# bloom-code-evaluation
Evaluation of [BLOOM](https://huggingface.co/bigscience/bloom) on the task of code generation using the [HumanEval](https://huggingface.co/datasets/openai_humaneval) benchmark.

## On JZ
This generates code for the 164 prompts present in the benchmark (200 generations are made for each problem):
### Setup
`transformers`and `accelerate` are installed from source along with datasets, we also clone the HumanEval benchmark to use it OFFLINE.
```
bash setup.sh
```

### Code generation
This script generates code, you can increase the batch size if you have enough memory. This outputs two files `generations.json`and `references.json`. 
```
bash generate_code.sh
```

## On GCP
The `generations.json` and `references.json` files must be placed in the folder `output_file`. `HF_ALLOW_CODE_EVAL=1` allwos executing the code generated by the model. This prints the `pass@k`scores and saves them in eval_results.json

```python
python run_evaluation.py --HF_ALLOW_CODE_EVAL 1 --output_file code_generations
```
